{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM1: Logistic Regression.\n",
    "\n",
    "### Name: [Sri Naga Hansi Mamidi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also, you should plot their objective values versus epochs and compare their training and testing accuracy. You will need to tune the parameters a little bit to obtain reasonable results.\n",
    "\n",
    "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0    ...        25.380          17.33           184.60      2019.0   \n",
       "1    ...        24.990          23.41           158.80      1956.0   \n",
       "2    ...        23.570          25.53           152.50      1709.0   \n",
       "3    ...        14.910          26.50            98.87       567.7   \n",
       "4    ...        22.540          16.67           152.20      1575.0   \n",
       "..   ...           ...            ...              ...         ...   \n",
       "564  ...        25.450          26.40           166.10      2027.0   \n",
       "565  ...        23.690          38.25           155.00      1731.0   \n",
       "566  ...        18.980          34.12           126.70      1124.0   \n",
       "567  ...        25.740          39.42           184.60      1821.0   \n",
       "568  ...         9.456          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"data.csv\")\n",
    "data\n",
    "data.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "df=df.drop(['id'],axis=1)\n",
    "df=df.replace('B',1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>-1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           -1        17.99         10.38          122.80     1001.0   \n",
       "1           -1        20.57         17.77          132.90     1326.0   \n",
       "2           -1        19.69         21.25          130.00     1203.0   \n",
       "3           -1        11.42         20.38           77.58      386.1   \n",
       "4           -1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564         -1        21.56         22.39          142.00     1479.0   \n",
       "565         -1        20.13         28.25          131.20     1261.0   \n",
       "566         -1        16.60         28.08          108.30      858.1   \n",
       "567         -1        20.60         29.33          140.10     1265.0   \n",
       "568          1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.replace('M',-1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       " 68         9.029         17.33           58.79      250.5          0.10660   \n",
       " 181       21.090         26.57          142.70     1311.0          0.11410   \n",
       " 63         9.173         13.86           59.20      260.9          0.07721   \n",
       " 248       10.650         25.22           68.01      347.0          0.09657   \n",
       " 60        10.170         14.88           64.55      311.9          0.11340   \n",
       " ..           ...           ...             ...        ...              ...   \n",
       " 71         8.888         14.64           58.79      244.0          0.09783   \n",
       " 106       11.640         18.33           75.17      412.5          0.11420   \n",
       " 270       14.290         16.82           90.30      632.6          0.06429   \n",
       " 435       13.980         19.62           91.12      599.5          0.10600   \n",
       " 102       12.180         20.52           77.22      458.7          0.08013   \n",
       " \n",
       "      compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       " 68            0.14130         0.31300              0.04375         0.2111   \n",
       " 181           0.28320         0.24870              0.14960         0.2395   \n",
       " 63            0.08751         0.05988              0.02180         0.2341   \n",
       " 248           0.07234         0.02379              0.01615         0.1897   \n",
       " 60            0.08061         0.01084              0.01290         0.2743   \n",
       " ..                ...             ...                  ...            ...   \n",
       " 71            0.15310         0.08606              0.02872         0.1902   \n",
       " 106           0.10170         0.07070              0.03485         0.1801   \n",
       " 270           0.02675         0.00725              0.00625         0.1508   \n",
       " 435           0.11330         0.11260              0.06463         0.1669   \n",
       " 102           0.04038         0.02383              0.01770         0.1739   \n",
       " \n",
       "      fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       " 68                  0.08046  ...        10.310          22.65   \n",
       " 181                 0.07398  ...        26.680          33.48   \n",
       " 63                  0.06963  ...        10.010          19.23   \n",
       " 248                 0.06329  ...        12.250          35.19   \n",
       " 60                  0.06960  ...        11.020          17.45   \n",
       " ..                      ...  ...           ...            ...   \n",
       " 71                  0.08980  ...         9.733          15.67   \n",
       " 106                 0.06520  ...        13.140          29.26   \n",
       " 270                 0.05376  ...        14.910          20.65   \n",
       " 435                 0.06544  ...        17.040          30.80   \n",
       " 102                 0.05677  ...        13.340          32.84   \n",
       " \n",
       "      perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       " 68             65.50       324.7           0.14820            0.43650   \n",
       " 181           176.50      2089.0           0.14910            0.75840   \n",
       " 63             65.59       310.1           0.09836            0.16780   \n",
       " 248            77.98       455.7           0.14990            0.13980   \n",
       " 60             69.86       368.6           0.12750            0.09866   \n",
       " ..               ...         ...               ...                ...   \n",
       " 71             62.56       284.4           0.12070            0.24360   \n",
       " 106            85.51       521.7           0.16880            0.26600   \n",
       " 270            94.44       684.6           0.08567            0.05036   \n",
       " 435           113.90       869.3           0.16130            0.35680   \n",
       " 102            84.58       547.8           0.11230            0.08862   \n",
       " \n",
       "      concavity_worst  concave points_worst  symmetry_worst  \\\n",
       " 68           1.25200               0.17500          0.4228   \n",
       " 181          0.67800               0.29030          0.4098   \n",
       " 63           0.13970               0.05087          0.3282   \n",
       " 248          0.11250               0.06136          0.3409   \n",
       " 60           0.02168               0.02579          0.3557   \n",
       " ..               ...                   ...             ...   \n",
       " 71           0.14340               0.04786          0.2254   \n",
       " 106          0.28730               0.12180          0.2806   \n",
       " 270          0.03866               0.03333          0.2458   \n",
       " 435          0.40690               0.18270          0.3179   \n",
       " 102          0.11450               0.07431          0.2694   \n",
       " \n",
       "      fractal_dimension_worst  \n",
       " 68                   0.11750  \n",
       " 181                  0.12840  \n",
       " 63                   0.08490  \n",
       " 248                  0.08147  \n",
       " 60                   0.08020  \n",
       " ..                       ...  \n",
       " 71                   0.10840  \n",
       " 106                  0.09097  \n",
       " 270                  0.06120  \n",
       " 435                  0.10550  \n",
       " 102                  0.06878  \n",
       " \n",
       " [455 rows x 30 columns],\n",
       "      radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       " 204        12.47         18.60           81.09      481.9          0.09965   \n",
       " 70         18.94         21.31          123.60     1130.0          0.09009   \n",
       " 131        15.46         19.48          101.70      748.9          0.10920   \n",
       " 431        12.40         17.68           81.47      467.8          0.10540   \n",
       " 540        11.54         14.44           74.65      402.9          0.09984   \n",
       " ..           ...           ...             ...        ...              ...   \n",
       " 486        14.64         16.85           94.21      666.0          0.08641   \n",
       " 75         16.07         19.65          104.10      817.7          0.09168   \n",
       " 249        11.52         14.93           73.87      406.3          0.10130   \n",
       " 238        14.22         27.85           92.55      623.9          0.08223   \n",
       " 265        20.73         31.12          135.70     1419.0          0.09469   \n",
       " \n",
       "      compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       " 204           0.10580         0.08005              0.03821         0.1925   \n",
       " 70            0.10290         0.10800              0.07951         0.1582   \n",
       " 131           0.12230         0.14660              0.08087         0.1931   \n",
       " 431           0.13160         0.07741              0.02799         0.1811   \n",
       " 540           0.11200         0.06737              0.02594         0.1818   \n",
       " ..                ...             ...                  ...            ...   \n",
       " 486           0.06698         0.05192              0.02791         0.1409   \n",
       " 75            0.08424         0.09769              0.06638         0.1798   \n",
       " 249           0.07808         0.04328              0.02929         0.1883   \n",
       " 238           0.10390         0.11030              0.04408         0.1342   \n",
       " 265           0.11430         0.13670              0.08646         0.1769   \n",
       " \n",
       "      fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       " 204                 0.06373  ...         14.97          24.64   \n",
       " 70                  0.05461  ...         24.86          26.58   \n",
       " 131                 0.05796  ...         19.26          26.00   \n",
       " 431                 0.07102  ...         12.88          22.91   \n",
       " 540                 0.06782  ...         12.26          19.68   \n",
       " ..                      ...  ...           ...            ...   \n",
       " 486                 0.05355  ...         16.46          25.44   \n",
       " 75                  0.05391  ...         19.77          24.56   \n",
       " 249                 0.06168  ...         12.65          21.19   \n",
       " 238                 0.06129  ...         15.75          40.54   \n",
       " 265                 0.05674  ...         32.49          47.16   \n",
       " \n",
       "      perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       " 204            96.05       677.9            0.1426             0.2378   \n",
       " 70            165.90      1866.0            0.1193             0.2336   \n",
       " 131           124.90      1156.0            0.1546             0.2394   \n",
       " 431            89.61       515.8            0.1450             0.2629   \n",
       " 540            78.78       457.8            0.1345             0.2118   \n",
       " ..               ...         ...               ...                ...   \n",
       " 486           106.00       831.0            0.1142             0.2070   \n",
       " 75            128.80      1223.0            0.1500             0.2045   \n",
       " 249            80.88       491.8            0.1389             0.1582   \n",
       " 238           102.50       764.0            0.1081             0.2426   \n",
       " 265           214.00      3432.0            0.1401             0.2644   \n",
       " \n",
       "      concavity_worst  concave points_worst  symmetry_worst  \\\n",
       " 204           0.2671               0.10150          0.3014   \n",
       " 70            0.2687               0.17890          0.2551   \n",
       " 131           0.3791               0.15140          0.2837   \n",
       " 431           0.2403               0.07370          0.2556   \n",
       " 540           0.1797               0.06918          0.2329   \n",
       " ..               ...                   ...             ...   \n",
       " 486           0.2437               0.07828          0.2455   \n",
       " 75            0.2829               0.15200          0.2650   \n",
       " 249           0.1804               0.09608          0.2664   \n",
       " 238           0.3064               0.08219          0.1890   \n",
       " 265           0.3442               0.16590          0.2868   \n",
       " \n",
       "      fractal_dimension_worst  \n",
       " 204                  0.08750  \n",
       " 70                   0.06589  \n",
       " 131                  0.08019  \n",
       " 431                  0.09359  \n",
       " 540                  0.08134  \n",
       " ..                       ...  \n",
       " 486                  0.06596  \n",
       " 75                   0.06387  \n",
       " 249                  0.07809  \n",
       " 238                  0.07796  \n",
       " 265                  0.08218  \n",
       " \n",
       " [114 rows x 30 columns],\n",
       " 68     1\n",
       " 181   -1\n",
       " 63     1\n",
       " 248    1\n",
       " 60     1\n",
       "       ..\n",
       " 71     1\n",
       " 106    1\n",
       " 270    1\n",
       " 435   -1\n",
       " 102    1\n",
       " Name: diagnosis, Length: 455, dtype: int64,\n",
       " 204    1\n",
       " 70    -1\n",
       " 131   -1\n",
       " 431    1\n",
       " 540    1\n",
       "       ..\n",
       " 486    1\n",
       " 75    -1\n",
       " 249    1\n",
       " 238    1\n",
       " 265   -1\n",
       " Name: diagnosis, Length: 114, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machine learning.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop('diagnosis',axis=1), \n",
    "                                                    df['diagnosis'], test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# n1=x_train.shape[0]\n",
    "# n2=x_test.shape[0]\n",
    "# y_train=np.array(y_train).reshape(n1,1)\n",
    "# y_test=np.array(y_test).reshape(n2,1)\n",
    "# x_train.shape,x_test.shape,y_train.shape,y_test.shape\n",
    "x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to transform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "radius_mean                0.013646\n",
      "texture_mean               0.122535\n",
      "perimeter_mean             0.017830\n",
      "area_mean                  0.007201\n",
      "smoothness_mean            0.221149\n",
      "compactness_mean           0.068722\n",
      "concavity_mean            -0.006214\n",
      "concave points_mean        0.083925\n",
      "symmetry_mean              0.011485\n",
      "fractal_dimension_mean     0.028301\n",
      "radius_se                  0.055695\n",
      "texture_se                 0.130663\n",
      "perimeter_se               0.018831\n",
      "area_se                    0.028115\n",
      "smoothness_se              0.084815\n",
      "compactness_se            -0.042242\n",
      "concavity_se              -0.144548\n",
      "concave points_se         -0.077631\n",
      "symmetry_se               -0.019083\n",
      "fractal_dimension_se      -0.045792\n",
      "radius_worst               0.035400\n",
      "texture_worst              0.116600\n",
      "perimeter_worst            0.023669\n",
      "area_worst                 0.031653\n",
      "smoothness_worst           0.181063\n",
      "compactness_worst          0.049100\n",
      "concavity_worst           -0.057414\n",
      "concave points_worst       0.032431\n",
      "symmetry_worst            -0.033755\n",
      "fractal_dimension_worst    0.021828\n",
      "dtype: float64\n",
      "test std = \n",
      "radius_mean                0.984314\n",
      "texture_mean               1.035636\n",
      "perimeter_mean             0.996238\n",
      "area_mean                  0.957757\n",
      "smoothness_mean            1.031830\n",
      "compactness_mean           1.031378\n",
      "concavity_mean             1.016787\n",
      "concave points_mean        1.092371\n",
      "symmetry_mean              0.987689\n",
      "fractal_dimension_mean     0.892528\n",
      "radius_se                  0.891957\n",
      "texture_se                 1.082929\n",
      "perimeter_se               0.875660\n",
      "area_se                    0.799617\n",
      "smoothness_se              0.905555\n",
      "compactness_se             0.795486\n",
      "concavity_se               0.628685\n",
      "concave points_se          0.895874\n",
      "symmetry_se                1.057210\n",
      "fractal_dimension_se       0.712388\n",
      "radius_worst               1.023190\n",
      "texture_worst              1.061059\n",
      "perimeter_worst            1.034493\n",
      "area_worst                 1.015418\n",
      "smoothness_worst           0.931825\n",
      "compactness_worst          1.072842\n",
      "concavity_worst            0.981183\n",
      "concave points_worst       1.031324\n",
      "symmetry_worst             0.894602\n",
      "fractal_dimension_worst    1.059151\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).values.reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).values.reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.concatenate((x_train,np.ones((x_train.shape[0],1))),axis=1)\n",
    "x_train.shape\n",
    "x_test=np.concatenate((x_test,np.ones((x_test.shape[0],1))),axis=1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistic regression and when $\\lambda > 0$, it essentially becomes a regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value, or loss\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     objective function value, or loss (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    yxTw=np.dot(np.multiply(y,x),w)\n",
    "    mean=np.mean(np.log(1+np.exp(-yxTw)))\n",
    "    log_reg=(lam/2)*np.square(np.sum(w))\n",
    "    loss=mean+log_reg\n",
    "    return loss\n",
    "# pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective function value/loss: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "d=x_train.shape[1]\n",
    "w=np.zeros((d,1))\n",
    "y_train=np.array(y_train).reshape(x_train.shape[0],1)\n",
    "lam=0\n",
    "loss=objective(w,x_train,y_train,lam)\n",
    "print(\"Objective function value/loss: \"+str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "def gradient(w, x, y, lam):\n",
    "    yx=np.multiply(y,x)\n",
    "    yxTw=np.dot(yx,w)\n",
    "    mean=-np.mean(np.divide(yx,1+np.exp(yxTw)),axis=0).reshape(x.shape[1],1)\n",
    "#     print(mean.shape)\n",
    "    g=mean+lam*w\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative processes (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "\n",
    "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    objvals=np.zeros(max_epoch)\n",
    "    for i in range(max_epoch):\n",
    "        objective_value=objective(w,x,y,lam)\n",
    "        objvals[i]=objective_value\n",
    "        grad=gradient(w,x,y,lam)\n",
    "        w-=learning_rate*grad\n",
    "        \n",
    "    return w,objvals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.37669549],\n",
       "        [-0.3563752 ],\n",
       "        [-0.37310921],\n",
       "        [-0.37318216],\n",
       "        [-0.15031546],\n",
       "        [-0.12869813],\n",
       "        [-0.29018185],\n",
       "        [-0.39822207],\n",
       "        [-0.09644155],\n",
       "        [ 0.14998635],\n",
       "        [-0.32911769],\n",
       "        [-0.00879974],\n",
       "        [-0.27638172],\n",
       "        [-0.29673247],\n",
       "        [-0.000952  ],\n",
       "        [ 0.11577579],\n",
       "        [ 0.09112133],\n",
       "        [-0.04458518],\n",
       "        [ 0.07284136],\n",
       "        [ 0.19383092],\n",
       "        [-0.44542195],\n",
       "        [-0.43548073],\n",
       "        [-0.42545609],\n",
       "        [-0.42059207],\n",
       "        [-0.30095308],\n",
       "        [-0.20714363],\n",
       "        [-0.28923018],\n",
       "        [-0.39881884],\n",
       "        [-0.30809827],\n",
       "        [-0.09045903],\n",
       "        [ 0.35416251]]),\n",
       " array([0.69314718, 0.52753622, 0.44115723, 0.38771795, 0.35076929,\n",
       "        0.32334696, 0.30197895, 0.28472811, 0.27042246, 0.25830813,\n",
       "        0.24787625, 0.23876973, 0.23072971, 0.22356345, 0.217124  ,\n",
       "        0.21129712, 0.20599239, 0.20113715, 0.19667222, 0.19254883,\n",
       "        0.18872634, 0.18517056, 0.18185248, 0.17874732, 0.17583372,\n",
       "        0.1730932 , 0.17050964, 0.16806893, 0.16575865, 0.16356781,\n",
       "        0.16148668, 0.15950657, 0.15761971, 0.15581914, 0.15409857,\n",
       "        0.15245236, 0.15087538, 0.14936298, 0.14791093, 0.14651537,\n",
       "        0.14517278, 0.14387993, 0.14263386, 0.14143185, 0.14027137,\n",
       "        0.13915012, 0.13806596, 0.13701691, 0.13600111, 0.13501688,\n",
       "        0.13406261, 0.13313683, 0.13223816, 0.13136531, 0.13051707,\n",
       "        0.12969232, 0.12888999, 0.1281091 , 0.12734871, 0.12660795,\n",
       "        0.12588599, 0.12518204, 0.12449539, 0.12382533, 0.12317121,\n",
       "        0.1225324 , 0.12190834, 0.12129845, 0.12070221, 0.12011913,\n",
       "        0.11954872, 0.11899054, 0.11844416, 0.11790917, 0.11738518,\n",
       "        0.11687181, 0.11636873, 0.11587559, 0.11539206, 0.11491784,\n",
       "        0.11445265, 0.11399618, 0.11354819, 0.1131084 , 0.11267658,\n",
       "        0.11225248, 0.11183588, 0.11142656, 0.11102432, 0.11062894,\n",
       "        0.11024025, 0.10985805, 0.10948217, 0.10911243, 0.10874867,\n",
       "        0.10839073, 0.10803846, 0.1076917 , 0.10735033, 0.1070142 ]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam=0\n",
    "learning_rate=0.1\n",
    "w=np.zeros((d,1))\n",
    "w_gd, objvals_gd=gradient_descent(x_train,y_train,lam,learning_rate,w)\n",
    "w_gd, objvals_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.11586941],\n",
       "        [-0.08104872],\n",
       "        [-0.11649772],\n",
       "        [-0.11092322],\n",
       "        [-0.05210598],\n",
       "        [-0.07173832],\n",
       "        [-0.09577818],\n",
       "        [-0.12109147],\n",
       "        [-0.04422352],\n",
       "        [ 0.0250556 ],\n",
       "        [-0.08348772],\n",
       "        [ 0.00324759],\n",
       "        [-0.0770225 ],\n",
       "        [-0.07752544],\n",
       "        [ 0.0105651 ],\n",
       "        [-0.00642808],\n",
       "        [-0.0005375 ],\n",
       "        [-0.03616826],\n",
       "        [ 0.00882388],\n",
       "        [ 0.02605644],\n",
       "        [-0.1273182 ],\n",
       "        [-0.09481459],\n",
       "        [-0.12575557],\n",
       "        [-0.11750879],\n",
       "        [-0.07625779],\n",
       "        [-0.08313099],\n",
       "        [-0.09460971],\n",
       "        [-0.12426617],\n",
       "        [-0.08043447],\n",
       "        [-0.03970684],\n",
       "        [ 0.08668354]]),\n",
       " array([0.69314718, 0.73781281, 0.97910146, 1.21626511, 1.41461714,\n",
       "        1.57255292, 1.69579829, 1.79106234, 1.86434408, 1.92057513,\n",
       "        1.96366727, 1.99666965, 2.02193732, 2.04128027, 2.05608599,\n",
       "        2.06741669, 2.07608522, 2.08271361, 2.08777799, 2.09164304,\n",
       "        2.0945882 , 2.09682775, 2.09852611, 2.09980949, 2.10077488,\n",
       "        2.10149678, 2.10203249, 2.10242606, 2.10271138, 2.1029145 ,\n",
       "        2.10305543, 2.10314958, 2.10320873, 2.10324194, 2.10325612,\n",
       "        2.10325656, 2.10324727, 2.1032313 , 2.10321092, 2.10318785,\n",
       "        2.10316337, 2.10313841, 2.10311362, 2.10308949, 2.10306633,\n",
       "        2.10304436, 2.10302369, 2.10300439, 2.10298648, 2.10296992,\n",
       "        2.10295469, 2.10294073, 2.10292796, 2.10291632, 2.10290573,\n",
       "        2.10289611, 2.10288739, 2.1028795 , 2.10287236, 2.10286592,\n",
       "        2.10286011, 2.10285488, 2.10285017, 2.10284593, 2.10284213,\n",
       "        2.1028387 , 2.10283563, 2.10283288, 2.10283041, 2.10282819,\n",
       "        2.10282621, 2.10282444, 2.10282285, 2.10282142, 2.10282015,\n",
       "        2.10281901, 2.10281799, 2.10281709, 2.10281627, 2.10281555,\n",
       "        2.1028149 , 2.10281432, 2.1028138 , 2.10281334, 2.10281293,\n",
       "        2.10281256, 2.10281224, 2.10281194, 2.10281168, 2.10281145,\n",
       "        2.10281124, 2.10281106, 2.10281089, 2.10281075, 2.10281062,\n",
       "        2.1028105 , 2.1028104 , 2.1028103 , 2.10281022, 2.10281015]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w=np.zeros((d,1))\n",
    "lam=1\n",
    "learning_rate=0.1\n",
    "w_gd_reg, objvals_gd_reg=gradient_descent(x_train,y_train,lam,learning_rate,w)\n",
    "w_gd_reg, objvals_gd_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    yixi=np.multiply(yi,xi)\n",
    "    yxTw=np.dot(yixi,w)\n",
    "    log_term=np.log(1+np.exp(-yxTw))\n",
    "    second_term=(lam/2)*np.sum(w*w)\n",
    "    obj=log_term+second_term\n",
    "    g=-yixi/(1+np.exp(yxTw))+(lam*w).reshape(xi.shape[0])\n",
    "    \n",
    "#     yxTw=np.dot(yx,w)\n",
    "#     mean=np.array(-np.mean(np.divide(yx,1+np.exp(yxTw)),axis=0)).reshape(d,1)\n",
    "#     print(mean.shape)\n",
    "    return obj,g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples.\n",
    "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    objvals=np.zeros(max_epoch)\n",
    "    for i in range(max_epoch):\n",
    "        rp=np.random.permutation(x.shape[0])\n",
    "        x_sample=x[rp,:]\n",
    "        y_sample=y[rp,:]\n",
    "        objective_value=0\n",
    "        for j in range(x.shape[0]):\n",
    "            xi=x_sample[j,:]\n",
    "            yi=float(y_sample[j,:])\n",
    "            objective,sg=stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objective_value+=objective\n",
    "#             print(sg,sg.shape)\n",
    "            w-=learning_rate*(sg.reshape((x.shape[1],1)))\n",
    "#         learning_rate*=0.9\n",
    "        objective_value/=x.shape[0]\n",
    "        objvals[i]=objective_value\n",
    "        \n",
    "    return w,objvals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9.28741793e-03],\n",
       "        [ 3.33133880e-01],\n",
       "        [ 3.47234534e-01],\n",
       "        [-3.28025328e-02],\n",
       "        [-2.10899694e-01],\n",
       "        [ 3.48193005e+00],\n",
       "        [-1.96666822e+00],\n",
       "        [-4.08813710e+00],\n",
       "        [ 1.21926377e+00],\n",
       "        [-1.29238503e+00],\n",
       "        [-4.82647733e+00],\n",
       "        [ 7.09253668e-01],\n",
       "        [ 1.61746425e-02],\n",
       "        [-3.42436239e+00],\n",
       "        [-1.26073146e+00],\n",
       "        [-1.17189542e-01],\n",
       "        [ 2.81620932e+00],\n",
       "        [-1.37648833e+00],\n",
       "        [ 1.48432254e+00],\n",
       "        [ 1.93342295e+00],\n",
       "        [-2.26759745e+00],\n",
       "        [-3.31952229e+00],\n",
       "        [-3.83373111e-03],\n",
       "        [-2.33174247e+00],\n",
       "        [ 3.29849523e-01],\n",
       "        [-9.90649054e-03],\n",
       "        [-3.36814465e+00],\n",
       "        [-1.30541802e+00],\n",
       "        [-3.68161439e+00],\n",
       "        [ 1.12838785e-01],\n",
       "        [-4.78879429e-01]]),\n",
       " array([0.11721372, 0.08408178, 0.07117451, 0.06610834, 0.06417085,\n",
       "        0.06033046, 0.06227117, 0.06038403, 0.05488111, 0.05520044,\n",
       "        0.05757832, 0.05792257, 0.05514128, 0.0556715 , 0.05269596,\n",
       "        0.05702046, 0.05183891, 0.05051356, 0.05680388, 0.05559622,\n",
       "        0.05177653, 0.05658663, 0.05483344, 0.0509076 , 0.05167714,\n",
       "        0.05033674, 0.04859438, 0.0472327 , 0.05195361, 0.04921104,\n",
       "        0.05002867, 0.04879809, 0.05568693, 0.04950753, 0.04936017,\n",
       "        0.04980624, 0.04550937, 0.04540724, 0.04767773, 0.04506568,\n",
       "        0.05673997, 0.04859791, 0.04519813, 0.05030012, 0.04473215,\n",
       "        0.04508121, 0.04865062, 0.04396603, 0.04945317, 0.05096485,\n",
       "        0.04618692, 0.04382658, 0.04508762, 0.04306352, 0.04332831,\n",
       "        0.04321982, 0.04840901, 0.04330136, 0.0418386 , 0.04437135,\n",
       "        0.04355706, 0.05028933, 0.04609678, 0.04755801, 0.04316394,\n",
       "        0.04304815, 0.04312142, 0.04282085, 0.04140946, 0.04439758,\n",
       "        0.04171067, 0.04108269, 0.04164631, 0.04258756, 0.04142315,\n",
       "        0.0430704 , 0.04349816, 0.04104814, 0.04041911, 0.04071174,\n",
       "        0.04077105, 0.03753902, 0.03903096, 0.04136629, 0.03978928,\n",
       "        0.03936694, 0.04180443, 0.04239748, 0.0436877 , 0.03953988,\n",
       "        0.03966851, 0.0381398 , 0.0409034 , 0.04461413, 0.03805608,\n",
       "        0.04125761, 0.04047   , 0.03891553, 0.03972106, 0.04327746]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w=np.zeros((d,1))\n",
    "lam=0\n",
    "learning_rate=0.1\n",
    "# print(x_train)\n",
    "w_sgd, objvals_sgd=sgd(x_train,y_train,lam,learning_rate,w)\n",
    "w_sgd, objvals_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.13641452],\n",
       "        [-0.13225883],\n",
       "        [-0.13701156],\n",
       "        [-0.12416501],\n",
       "        [-0.05509678],\n",
       "        [-0.06658489],\n",
       "        [-0.10461083],\n",
       "        [-0.12832951],\n",
       "        [-0.05658207],\n",
       "        [ 0.03173381],\n",
       "        [-0.11099639],\n",
       "        [-0.05674831],\n",
       "        [-0.10414567],\n",
       "        [-0.09695629],\n",
       "        [ 0.0450215 ],\n",
       "        [ 0.00427912],\n",
       "        [-0.01277168],\n",
       "        [-0.06019017],\n",
       "        [-0.03793616],\n",
       "        [-0.00938595],\n",
       "        [-0.14568233],\n",
       "        [-0.14544421],\n",
       "        [-0.14232727],\n",
       "        [-0.13095772],\n",
       "        [-0.04823597],\n",
       "        [-0.05586767],\n",
       "        [-0.10185443],\n",
       "        [-0.13305737],\n",
       "        [-0.08402823],\n",
       "        [-0.04179006],\n",
       "        [ 0.14299844]]),\n",
       " array([0.48388543, 0.46920007, 0.49970913, 0.47511157, 0.49659328,\n",
       "        0.4603292 , 0.46560011, 0.49304393, 0.47842626, 0.46953841,\n",
       "        0.4523968 , 0.47715286, 0.4860436 , 0.47041383, 0.4933956 ,\n",
       "        0.47704057, 0.47980319, 0.47900186, 0.45510499, 0.47062296,\n",
       "        0.47674306, 0.48900877, 0.46949501, 0.47409894, 0.46098622,\n",
       "        0.46257897, 0.46342671, 0.49540982, 0.4904333 , 0.47666944,\n",
       "        0.49938238, 0.46964147, 0.4747151 , 0.47703141, 0.48604345,\n",
       "        0.48336326, 0.48625077, 0.4911498 , 0.47985102, 0.49139105,\n",
       "        0.46713693, 0.48686687, 0.48880102, 0.48096879, 0.48221938,\n",
       "        0.47008477, 0.47807325, 0.47917909, 0.49231321, 0.49574839,\n",
       "        0.47424416, 0.4967249 , 0.45904181, 0.48212964, 0.50026055,\n",
       "        0.46623978, 0.47741494, 0.47334828, 0.48470298, 0.46899169,\n",
       "        0.46158   , 0.47324865, 0.49126174, 0.49009592, 0.4701099 ,\n",
       "        0.47824152, 0.4656388 , 0.47001241, 0.49911278, 0.47363722,\n",
       "        0.48021616, 0.47441219, 0.4715304 , 0.48416408, 0.48567594,\n",
       "        0.49035492, 0.49369576, 0.47724939, 0.48030253, 0.47858509,\n",
       "        0.4865224 , 0.46655917, 0.47121633, 0.47911737, 0.49861261,\n",
       "        0.46077554, 0.48448122, 0.46715653, 0.47827842, 0.4750688 ,\n",
       "        0.4867988 , 0.49571979, 0.47652579, 0.4737838 , 0.46909549,\n",
       "        0.46082328, 0.47328109, 0.48213042, 0.4899339 , 0.47360957]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w=np.zeros((d,1))\n",
    "lam=1\n",
    "learning_rate=0.1\n",
    "w_sgd_reg, objvals_sgd_reg=sgd(x_train,y_train,lam,learning_rate,w)\n",
    "w_sgd_reg, objvals_sgd_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-b matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    yixi=np.multiply(yi,xi)\n",
    "    yxTw=np.dot(yixi,w)\n",
    "    obj=np.mean(np.log(1+np.exp(-yxTw)))+((lam/2)*np.sum(w*w))\n",
    "    g=np.mean(np.divide(-yixi,1+np.exp(yxTw)),axis=0).reshape(xi.shape[1],1)+(lam*w)\n",
    "    return obj,g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "#     w=np.zeros((d,1))\n",
    "    objvals=np.zeros(max_epoch)\n",
    "    b_size=10\n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        samples=np.random.permutation(x.shape[0])\n",
    "        x_sample=x[samples,:]\n",
    "        y_sample=y[samples,:]\n",
    "        batch=[]\n",
    "        for i in range(0,len(samples),b_size):\n",
    "            batch.append(samples[i:i+10])        \n",
    "        \n",
    "    for i in range(max_epoch):\n",
    "        objective_value=0\n",
    "        for b in batch:\n",
    "            xi=x[b,:]\n",
    "            yi=y[b,:]\n",
    "#             print(xi.shape,yi.shape)\n",
    "            objective,gradient=mb_objective_gradient(w,xi,yi,lam)\n",
    "            objective_value+=objective\n",
    "            w-=learning_rate*gradient\n",
    "#         learning_rate*=0.9\n",
    "        objvals[i]=objective_value/(x.shape[0]/b_size)\n",
    "#     print(\"done\")\n",
    "    return w,objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.4884095 ],\n",
       "        [-0.48907711],\n",
       "        [-0.43764867],\n",
       "        [-0.57398492],\n",
       "        [-0.0951956 ],\n",
       "        [ 0.76882293],\n",
       "        [-0.99390738],\n",
       "        [-1.30953588],\n",
       "        [ 0.34868202],\n",
       "        [ 0.1624878 ],\n",
       "        [-1.68138727],\n",
       "        [ 0.22025723],\n",
       "        [-0.91297988],\n",
       "        [-1.21311347],\n",
       "        [-0.42178966],\n",
       "        [ 0.99220386],\n",
       "        [ 0.09805528],\n",
       "        [-0.38187212],\n",
       "        [ 0.62784018],\n",
       "        [ 0.85524692],\n",
       "        [-1.11918375],\n",
       "        [-1.70877364],\n",
       "        [-0.79425325],\n",
       "        [-1.10108902],\n",
       "        [-0.82854225],\n",
       "        [ 0.02647405],\n",
       "        [-1.25038503],\n",
       "        [-1.06959807],\n",
       "        [-1.62111097],\n",
       "        [-0.21841015],\n",
       "        [ 0.39246739]]),\n",
       " array([0.22448904, 0.12311578, 0.10421457, 0.09448474, 0.08828256,\n",
       "        0.08387541, 0.08052621, 0.07786174, 0.07567053, 0.0738228 ,\n",
       "        0.072234  , 0.07084637, 0.06961896, 0.06852179, 0.06753235,\n",
       "        0.06663335, 0.06581127, 0.06505532, 0.06435681, 0.0637086 ,\n",
       "        0.06310481, 0.06254048, 0.06201145, 0.06151416, 0.06104556,\n",
       "        0.06060299, 0.06018416, 0.05978705, 0.05940987, 0.05905105,\n",
       "        0.05870917, 0.05838298, 0.05807134, 0.05777323, 0.05748772,\n",
       "        0.05721398, 0.05695122, 0.05669875, 0.05645592, 0.05622214,\n",
       "        0.05599686, 0.05577957, 0.05556982, 0.05536715, 0.05517118,\n",
       "        0.05498153, 0.05479785, 0.05461982, 0.05444714, 0.05427952,\n",
       "        0.0541167 , 0.05395844, 0.05380451, 0.05365469, 0.05350879,\n",
       "        0.0533666 , 0.05322797, 0.05309272, 0.05296069, 0.05283174,\n",
       "        0.05270574, 0.05258255, 0.05246206, 0.05234414, 0.05222869,\n",
       "        0.05211562, 0.05200482, 0.0518962 , 0.05178968, 0.05168517,\n",
       "        0.05158261, 0.05148192, 0.05138302, 0.05128586, 0.05119037,\n",
       "        0.0510965 , 0.05100419, 0.05091338, 0.05082403, 0.05073608,\n",
       "        0.0506495 , 0.05056423, 0.05048024, 0.05039749, 0.05031594,\n",
       "        0.05023555, 0.05015629, 0.05007813, 0.05000102, 0.04992495,\n",
       "        0.04984989, 0.0497758 , 0.04970266, 0.04963044, 0.04955913,\n",
       "        0.04948869, 0.0494191 , 0.04935034, 0.04928239, 0.04921523]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w=np.zeros((d,1))\n",
    "lam=0\n",
    "learning_rate=0.1\n",
    "w_mbgd, objvals_mbgd=mbgd(x_train, y_train, lam, learning_rate, w)\n",
    "w_mbgd, objvals_mbgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.12349145],\n",
       "        [-0.08784362],\n",
       "        [-0.12336693],\n",
       "        [-0.118514  ],\n",
       "        [-0.01873789],\n",
       "        [-0.06171432],\n",
       "        [-0.09991196],\n",
       "        [-0.11380343],\n",
       "        [-0.04886009],\n",
       "        [ 0.03871227],\n",
       "        [-0.07962726],\n",
       "        [ 0.00793765],\n",
       "        [-0.07349044],\n",
       "        [-0.07556986],\n",
       "        [ 0.01863997],\n",
       "        [-0.01906528],\n",
       "        [-0.02375345],\n",
       "        [-0.04118143],\n",
       "        [ 0.01518688],\n",
       "        [ 0.00548835],\n",
       "        [-0.13038878],\n",
       "        [-0.09393603],\n",
       "        [-0.12910681],\n",
       "        [-0.12042238],\n",
       "        [-0.04594107],\n",
       "        [-0.08155996],\n",
       "        [-0.09723812],\n",
       "        [-0.12234899],\n",
       "        [-0.06981909],\n",
       "        [-0.03408947],\n",
       "        [ 0.08994572]]),\n",
       " array([0.43664017, 0.42483671, 0.42483979, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981,\n",
       "        0.42483981, 0.42483981, 0.42483981, 0.42483981, 0.42483981]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w=np.zeros((d,1))\n",
    "lam=1\n",
    "learning_rate=0.1\n",
    "w_mbgd_reg, objvals_mbgd_reg=mbgd(x_train, y_train, lam, learning_rate, w)\n",
    "w_mbgd_reg, objvals_mbgd_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epoch_gd=range(len(objvals_gd))\n",
    "epoch_sgd=range(len(objvals_sgd))\n",
    "epoch_mbgd=range(len(objvals_mbgd))\n",
    "\n",
    "plot1,=plt.plot(epoch_gd,objvals_gd)\n",
    "plot2,=plt.plot(epoch_sgd,objvals_sgd)\n",
    "plot3,=plt.plot(epoch_mbgd,objvals_mbgd)\n",
    "plt.title(\"Objective Function Values vs Epochs\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Objective Function Value')\n",
    "plt.legend([plot1,plot2,plot3],['Gradient Descent','Stochastic Gradient Descent','Mini-Batch Gradient descent'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    f=np.sign(np.dot(X,w))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Accuracy: 0.9824175824175824\n",
      "Regularized Gradient Descent Accuracy: 0.9582417582417583\n",
      "0.01098901098901099\n",
      "Stochastic Gradient Descent Accuracy: 0.989010989010989\n",
      "Regularized Stochastic Gradient Descent Accuracy: 0.9472527472527472\n",
      "Mini-Batch Gradient Descent Accuracy: 0.9868131868131869\n",
      "Regularized Mini-Batch Gradient Descent Accuracy: 0.9582417582417583\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistic regression and regularized version\n",
    "# evaluate training error of logistic regression and regularized version\n",
    "\n",
    "#Gradient Descent\n",
    "predict_train=predict(w_gd,x_train)\n",
    "error_gd=np.mean(np.abs(predict_train-y_train)/2)\n",
    "# print(predict_train.shape,y_train.shape)\n",
    "print(\"Gradient Descent Accuracy: \"+str(1-error_gd))\n",
    "\n",
    "predict_train_reg=predict(w_gd_reg,x_train)\n",
    "error_gd_reg=np.mean(np.abs(predict_train_reg-y_train)/2)\n",
    "print(\"Regularized Gradient Descent Accuracy: \"+str(1-error_gd_reg))\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "predict_train=predict(w_sgd,x_train)\n",
    "error_sgd=np.mean(np.abs(predict_train-y_train)/2)\n",
    "print(error_sgd)\n",
    "print(\"Stochastic Gradient Descent Accuracy: \"+str(1-error_sgd))\n",
    "\n",
    "predict_train_reg=predict(w_sgd_reg,x_train)\n",
    "error_sgd_reg=np.mean(np.abs(predict_train_reg-y_train)/2)\n",
    "print(\"Regularized Stochastic Gradient Descent Accuracy: \"+str(1-error_sgd_reg))\n",
    "\n",
    "#Mini-Batch Gradient Descent\n",
    "predict_train=predict(w_mbgd,x_train)\n",
    "error_mbgd=np.mean(np.abs(predict_train-y_train)/2)\n",
    "print(\"Mini-Batch Gradient Descent Accuracy: \"+str(1-error_mbgd))\n",
    "\n",
    "predict_train_reg=predict(w_gd_reg,x_train)\n",
    "error_mbgd_reg=np.mean(np.abs(predict_train_reg-y_train)/2)\n",
    "print(\"Regularized Mini-Batch Gradient Descent Accuracy: \"+str(1-error_mbgd_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Accuracy: 0.9824561403508771\n",
      "Regularized Gradient Descent Accuracy: 0.9736842105263158\n",
      "Stochastic Gradient Descent Accuracy: 0.9649122807017544\n",
      "Regularized Stochastic Gradient Descent Accuracy: 0.9736842105263158\n",
      "Mini-Batch Gradient Descent Accuracy: 0.9824561403508771\n",
      "Regularized Mini-Batch Gradient Descent Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistic regression and regularized version\n",
    "# evaluate testing error of logistic regression and regularized version\n",
    "y_test=np.array(y_test).reshape((y_test.shape[0],1))\n",
    "#Gradient Descent\n",
    "predict_test=predict(w_gd,x_test)\n",
    "# print(predict_test.shape,y_test.shape)\n",
    "error_gd=np.mean(np.abs(predict_test-y_test)/2)\n",
    "print(\"Gradient Descent Accuracy: \"+str(1-error_gd))\n",
    "\n",
    "predict_test_reg=predict(w_gd_reg,x_test)\n",
    "error_gd_reg=np.mean(np.abs(predict_test_reg-y_test)/2)\n",
    "print(\"Regularized Gradient Descent Accuracy: \"+str(1-error_gd_reg))\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "predict_test=predict(w_sgd,x_test)\n",
    "error_sgd=np.mean(np.abs(predict_test-y_test)/2)\n",
    "print(\"Stochastic Gradient Descent Accuracy: \"+str(1-error_sgd))\n",
    "\n",
    "predict_test_reg=predict(w_sgd_reg,x_test)\n",
    "error_sgd_reg=np.mean(np.abs(predict_test_reg-y_test)/2)\n",
    "print(\"Regularized Stochastic Gradient Descent Accuracy: \"+str(1-error_sgd_reg))\n",
    "\n",
    "#Mini-Batch Gradient Descent\n",
    "predict_test=predict(w_mbgd,x_test)\n",
    "error_mbgd=np.mean(np.abs(predict_test-y_test)/2)\n",
    "print(\"Mini-Batch Gradient Descent Accuracy: \"+str(1-error_mbgd))\n",
    "\n",
    "predict_test_reg=predict(w_gd_reg,x_test)\n",
    "error_mbgd_reg=np.mean(np.abs(predict_test_reg-y_test)/2)\n",
    "print(\"Regularized Mini-Batch Gradient Descent Accuracy: \"+str(1-error_mbgd_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 993.72230403],\n",
      "       [ 996.24968617],\n",
      "       [ 993.51479117],\n",
      "       [ 993.8722344 ],\n",
      "       [ 995.79787622],\n",
      "       [ 993.42137195],\n",
      "       [ 992.88775773],\n",
      "       [ 992.4294363 ],\n",
      "       [ 995.98610273],\n",
      "       [ 998.5465032 ],\n",
      "       [ 994.85698527],\n",
      "       [ 999.43137329],\n",
      "       [ 994.85304084],\n",
      "       [ 995.21286225],\n",
      "       [ 999.51911137],\n",
      "       [ 995.9673959 ],\n",
      "       [ 996.50341364],\n",
      "       [ 995.17724866],\n",
      "       [ 999.22394625],\n",
      "       [ 997.74738815],\n",
      "       [ 993.26091717],\n",
      "       [ 995.86006138],\n",
      "       [ 993.08443064],\n",
      "       [ 993.58913718],\n",
      "       [ 995.56340549],\n",
      "       [ 993.76618551],\n",
      "       [ 993.25975211],\n",
      "       [ 992.36544739],\n",
      "       [ 995.83802037],\n",
      "       [ 995.879253  ],\n",
      "       [1002.02989011]]), array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\3922307177.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  mean=np.mean(np.log(1+np.exp(-yxTw)))\n",
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\4092425742.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  mean=-np.mean(np.divide(yx,1+np.exp(yxTw)),axis=0).reshape(x.shape[1],1)\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent(x_train, y_train, 0, 0.1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.08086646],\n",
      "       [-0.05340548],\n",
      "       [-0.08159021],\n",
      "       [-0.07713448],\n",
      "       [-0.03781971],\n",
      "       [-0.05511567],\n",
      "       [-0.0691749 ],\n",
      "       [-0.08477704],\n",
      "       [-0.03328449],\n",
      "       [ 0.01324279],\n",
      "       [-0.0572008 ],\n",
      "       [ 0.00230959],\n",
      "       [-0.0538072 ],\n",
      "       [-0.05339496],\n",
      "       [ 0.0086749 ],\n",
      "       [-0.01175999],\n",
      "       [-0.00771201],\n",
      "       [-0.03064288],\n",
      "       [ 0.00484111],\n",
      "       [ 0.01142997],\n",
      "       [-0.0878646 ],\n",
      "       [-0.06185244],\n",
      "       [-0.08738752],\n",
      "       [-0.08109968],\n",
      "       [-0.05142394],\n",
      "       [-0.0608145 ],\n",
      "       [-0.06821042],\n",
      "       [-0.08746514],\n",
      "       [-0.05395631],\n",
      "       [-0.02991435],\n",
      "       [ 0.05206703]]), array([           inf,            inf,            inf,            inf,\n",
      "                  inf,            inf,            inf,            inf,\n",
      "                  inf,            inf,            inf,            inf,\n",
      "                  inf,            inf,            inf,            inf,\n",
      "                  inf,            inf,            inf,            inf,\n",
      "                  inf, 7.79527953e+04, 4.92853275e+04, 3.10622965e+04,\n",
      "       1.94988789e+04, 1.21777535e+04, 7.55581038e+03, 4.64864362e+03,\n",
      "       2.82878016e+03, 1.69665897e+03, 9.98178638e+02, 5.72028993e+02,\n",
      "       3.16029862e+02, 1.65638103e+02, 8.02453450e+01, 3.44192507e+01,\n",
      "       1.22132250e+01, 3.32560298e+00, 7.95217930e-01, 4.58982039e-01,\n",
      "       6.84311373e-01, 1.00014090e+00, 1.28379503e+00, 1.51161907e+00,\n",
      "       1.68738182e+00, 1.82094515e+00, 1.92198769e+00, 1.99845451e+00,\n",
      "       2.05646711e+00, 2.10062460e+00, 2.13435177e+00, 2.16019627e+00,\n",
      "       2.18005846e+00, 2.19536214e+00, 2.20717946e+00, 2.21632174e+00,\n",
      "       2.22340573e+00, 2.22890223e+00, 2.23317189e+00, 2.23649181e+00,\n",
      "       2.23907543e+00, 2.24108754e+00, 2.24265559e+00, 2.24387829e+00,\n",
      "       2.24483220e+00, 2.24557677e+00, 2.24615819e+00, 2.24661240e+00,\n",
      "       2.24696736e+00, 2.24724487e+00, 2.24746190e+00, 2.24763168e+00,\n",
      "       2.24776455e+00, 2.24786855e+00, 2.24794999e+00, 2.24801378e+00,\n",
      "       2.24806376e+00, 2.24810293e+00, 2.24813363e+00, 2.24815770e+00,\n",
      "       2.24817658e+00, 2.24819139e+00, 2.24820301e+00, 2.24821214e+00,\n",
      "       2.24821930e+00, 2.24822492e+00, 2.24822934e+00, 2.24823281e+00,\n",
      "       2.24823553e+00, 2.24823767e+00, 2.24823936e+00, 2.24824068e+00,\n",
      "       2.24824172e+00, 2.24824254e+00, 2.24824319e+00, 2.24824369e+00,\n",
      "       2.24824409e+00, 2.24824441e+00, 2.24824465e+00, 2.24824485e+00]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\3922307177.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  mean=np.mean(np.log(1+np.exp(-yxTw)))\n",
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\4092425742.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  mean=-np.mean(np.divide(yx,1+np.exp(yxTw)),axis=0).reshape(x.shape[1],1)\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent(x_train, y_train, 2, 0.1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\3946927642.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  obj=np.mean(np.log(1+np.exp(-yxTw)))+((lam/2)*np.sum(w*w))\n",
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\3946927642.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  g=np.mean(np.divide(-yixi,1+np.exp(yxTw)),axis=0).reshape(xi.shape[1],1)+(lam*w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 718.57978314],\n",
      "       [ 829.42632752],\n",
      "       [ 708.84192436],\n",
      "       [ 725.06370895],\n",
      "       [ 806.65146029],\n",
      "       [ 700.0510865 ],\n",
      "       [ 676.65629041],\n",
      "       [ 656.13895847],\n",
      "       [ 812.55683326],\n",
      "       [ 930.48032811],\n",
      "       [ 767.20815937],\n",
      "       [ 966.82694109],\n",
      "       [ 766.60564054],\n",
      "       [ 784.10955287],\n",
      "       [ 971.68987363],\n",
      "       [ 815.61938006],\n",
      "       [ 841.44728786],\n",
      "       [ 779.79954374],\n",
      "       [ 954.75718837],\n",
      "       [ 895.79249601],\n",
      "       [ 696.99049923],\n",
      "       [ 809.20548853],\n",
      "       [ 688.64541551],\n",
      "       [ 711.69286961],\n",
      "       [ 795.36058792],\n",
      "       [ 715.74452355],\n",
      "       [ 694.05208843],\n",
      "       [ 652.58711129],\n",
      "       [ 804.46965964],\n",
      "       [ 810.14228021],\n",
      "       [1090.95266003]]), array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "       inf, inf, inf, inf, inf, inf, inf, inf, inf]))\n"
     ]
    }
   ],
   "source": [
    "print(mbgd(x_train, y_train, 0, 0.1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.07501062],\n",
      "       [-0.07602998],\n",
      "       [-0.07698624],\n",
      "       [-0.07271033],\n",
      "       [-0.0213771 ],\n",
      "       [-0.06049255],\n",
      "       [-0.07711342],\n",
      "       [-0.08357994],\n",
      "       [ 0.0062552 ],\n",
      "       [ 0.00060297],\n",
      "       [-0.04745587],\n",
      "       [-0.00366111],\n",
      "       [-0.05068998],\n",
      "       [-0.04988788],\n",
      "       [-0.00204516],\n",
      "       [-0.02769433],\n",
      "       [-0.01666866],\n",
      "       [-0.02778045],\n",
      "       [ 0.02015244],\n",
      "       [ 0.00318385],\n",
      "       [-0.08565064],\n",
      "       [-0.08783097],\n",
      "       [-0.08879176],\n",
      "       [-0.08015267],\n",
      "       [-0.05296954],\n",
      "       [-0.07896522],\n",
      "       [-0.08990927],\n",
      "       [-0.08800279],\n",
      "       [-0.04837104],\n",
      "       [-0.05466161],\n",
      "       [ 0.06673533]]), array([       inf, 0.5020084 , 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223,\n",
      "       0.50188223, 0.50188223, 0.50188223, 0.50188223, 0.50188223]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\3946927642.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  obj=np.mean(np.log(1+np.exp(-yxTw)))+((lam/2)*np.sum(w*w))\n",
      "C:\\Users\\mhans\\AppData\\Local\\Temp\\ipykernel_8280\\3946927642.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  g=np.mean(np.divide(-yixi,1+np.exp(yxTw)),axis=0).reshape(xi.shape[1],1)+(lam*w)\n"
     ]
    }
   ],
   "source": [
    "print(mbgd(x_train, y_train, 2, 0.1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
